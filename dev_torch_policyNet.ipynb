{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of policy-CEM-method\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "_C = CN()\n",
    "_C.SYSTEM = CN()\n",
    "_C.SYSTEM.DEVICES = []\n",
    "_C.SYSTEM.VIRTUAL_SCREEN = (600, 400)\n",
    "_C.SYSTEM.PRINT_EVERY = 10\n",
    "\n",
    "_C.AGENT = CN()\n",
    "_C.AGENT.MAX_N_STEPS = 500\n",
    "_C.AGENT.GAMMA = 1.0\n",
    "_C.AGENT.POPULATION_SIZE = 50\n",
    "_C.AGENT.ELITE_FRAC = 0.2\n",
    "\n",
    "_C.GAME_ENV = CN()\n",
    "_C.GAME_ENV.N_ITERATIONS = 500\n",
    "#_C.GAME_ENV.N_ITERATIONS = 5000 # mountaincar-v0 may need about 5k\n",
    "_C.GAME_ENV.NOISE_SIGMA = 0.5\n",
    "\n",
    "def get_cfg_defaults():\n",
    "    return _C.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "from pyvirtualdisplay import Display\n",
    "from collections import deque\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "#from cartpole.config import get_cfg_defaults\n",
    "#from cartpole.utils import ReplayMemory, screen_to_state\n",
    "cfg = get_cfg_defaults()\n",
    "\n",
    "devices = \",\".join(str(i) for i in cfg.SYSTEM.DEVICES)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "\n",
    "torch_devices = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=cfg.SYSTEM.VIRTUAL_SCREEN)\n",
    "display.start()\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "#env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Agent_PolicyCEM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, device=\"cpu\"):\n",
    "        super(Agent_PolicyCEM, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, 32)\n",
    "        self.layer2 = nn.Linear(32, 64)\n",
    "        self.layer3 = nn.Linear(64, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return F.softmax(self.layer3(x)) # action = 0 or 1 (discrete)\n",
    "    \n",
    "    \n",
    "    def get_weights_dim(self):\n",
    "        # return numbers of parameters of layers --> n in flatten form\n",
    "        return [len(param.reshape(-1).detach().numpy()) for param in self.parameters()]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        new_network_state = self._pair_weight_key_to_weight(weights=weights)\n",
    "        self.load_state_dict(new_network_state)\n",
    "    \n",
    "    def evaluate(self, env, weights, gamma=1.0, max_n_steps=5000):\n",
    "        self.set_weights(weights)\n",
    "        state = env.reset()\n",
    "        rewards = 0.\n",
    "        for t in range(max_n_steps):\n",
    "            state = torch.from_numpy(state).float().to(self.device)\n",
    "            action = self.forward(state).detach().numpy()\n",
    "            #action = int(np.round(action)) # cartpole-v0 only accept 0 or 1 (no float between 0-1)\n",
    "            action = np.argmax(action) # cartpole-v0 only accept 0 or 1 (no float between 0-1)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards += reward * gamma**t\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        return rewards\n",
    "        \n",
    "    \n",
    "    def _pair_weight_key_to_weight(self, weights):\n",
    "        \"\"\"pair key and weights\n",
    "        Args:\n",
    "          weights: list of 1D-vectors n * [n_weights]\n",
    "          \n",
    "        Returns: A reshaped, mapped state_dict\n",
    "        \"\"\"\n",
    "        keys = list(self.state_dict().keys())\n",
    "        original_weight_shape = [list(param.shape) for param in self.parameters()]\n",
    "        return OrderedDict({k:torch.from_numpy(v).reshape(s) for k, v, s in zip(keys, weights, original_weight_shape)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent_PolicyCEM(input_size=env.observation_space.shape[0], \n",
    "                        output_size=env.action_space.n, \n",
    "                        device=torch_devices).to(torch_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/pyttf/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 9.00\n",
      "Episode 11\tAverage Score: 51.73\n",
      "Episode 21\tAverage Score: 77.10\n",
      "Episode 31\tAverage Score: 97.55\n",
      "Episode 41\tAverage Score: 121.27\n",
      "Episode 51\tAverage Score: 136.71\n",
      "Episode 61\tAverage Score: 147.08\n",
      "Episode 71\tAverage Score: 154.54\n",
      "Episode 81\tAverage Score: 160.15\n",
      "Episode 91\tAverage Score: 164.53\n",
      "Episode 101\tAverage Score: 169.63\n",
      "Episode 111\tAverage Score: 184.03\n",
      "Episode 121\tAverage Score: 193.53\n",
      "Episode 131\tAverage Score: 199.48\n",
      "Episode 141\tAverage Score: 200.00\n",
      "Episode 151\tAverage Score: 200.00\n",
      "Episode 161\tAverage Score: 200.00\n",
      "Episode 171\tAverage Score: 200.00\n",
      "Episode 181\tAverage Score: 200.00\n",
      "Episode 191\tAverage Score: 200.00\n",
      "Episode 201\tAverage Score: 200.00\n",
      "Episode 211\tAverage Score: 200.00\n",
      "Episode 221\tAverage Score: 200.00\n",
      "Episode 231\tAverage Score: 200.00\n",
      "Episode 241\tAverage Score: 200.00\n",
      "Episode 251\tAverage Score: 200.00\n",
      "Episode 261\tAverage Score: 200.00\n",
      "Episode 271\tAverage Score: 200.00\n",
      "Episode 281\tAverage Score: 200.00\n",
      "Episode 291\tAverage Score: 200.00\n",
      "Episode 301\tAverage Score: 200.00\n",
      "Episode 311\tAverage Score: 200.00\n",
      "Episode 321\tAverage Score: 200.00\n",
      "Episode 331\tAverage Score: 200.00\n",
      "Episode 341\tAverage Score: 200.00\n",
      "Episode 351\tAverage Score: 200.00\n",
      "Episode 361\tAverage Score: 200.00\n",
      "Episode 371\tAverage Score: 200.00\n",
      "Episode 381\tAverage Score: 200.00\n",
      "Episode 391\tAverage Score: 200.00\n",
      "Episode 401\tAverage Score: 200.00\n",
      "Episode 411\tAverage Score: 200.00\n",
      "Episode 421\tAverage Score: 200.00\n",
      "Episode 431\tAverage Score: 200.00\n",
      "Episode 441\tAverage Score: 200.00\n",
      "Episode 451\tAverage Score: 200.00\n",
      "Episode 461\tAverage Score: 200.00\n",
      "Episode 471\tAverage Score: 200.00\n",
      "Episode 481\tAverage Score: 200.00\n",
      "Episode 491\tAverage Score: 200.00\n",
      "\n",
      "Finally!\tAverage Score: 200.00\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "n_elite=int(cfg.AGENT.POPULATION_SIZE * cfg.AGENT.ELITE_FRAC)\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores = []\n",
    "best_weight = [cfg.GAME_ENV.NOISE_SIGMA * np.random.randn(i) for i in agent.get_weights_dim()]\n",
    "\n",
    "niter = cfg.GAME_ENV.N_ITERATIONS+1\n",
    "#niter = 2\n",
    "for i_iteration in range(1, niter):\n",
    "    weights_pop = [[best_weight[i] + (cfg.GAME_ENV.NOISE_SIGMA * np.random.randn(j)) for i,j in enumerate(agent.get_weights_dim())] \\\n",
    "                    for _ in range(cfg.AGENT.POPULATION_SIZE)]\n",
    "    rewards = np.array([agent.evaluate(env=env, \n",
    "                                       weights=weights, \n",
    "                                       gamma=cfg.AGENT.GAMMA, \n",
    "                                       max_n_steps=cfg.AGENT.MAX_N_STEPS) for weights in weights_pop])\n",
    "\n",
    "    elite_idxs = rewards.argsort()[-n_elite:]\n",
    "    elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "    best_weight = np.array(elite_weights).mean(axis=0)\n",
    "\n",
    "    reward = agent.evaluate(env=env, weights=best_weight, gamma=cfg.AGENT.GAMMA, max_n_steps=1000)\n",
    "    scores_deque.append(reward)\n",
    "    scores.append(reward)\n",
    "\n",
    "    #torch.save(agent.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    if i_iteration % cfg.SYSTEM.PRINT_EVERY == 1:\n",
    "        print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "\n",
    "    \n",
    "print('\\nFinally!\\tAverage Score: {:.2f}'.format(np.mean(scores_deque)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training, we examine the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABDlJREFUeJzt3OFpIlEYQNGZxSZSh21sHaYmrWPbSB1bxuRPWDYxCwuOeS/Xc0BQQfh+jJeP4em6bdsCQM+P0QMAcB8CDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRh9EDvPFzWoBr6y0ftsEDRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QdRg9Az8vl+eq94+k8YBJ4bDZ4diXuMA+BB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogWc3n/1VMDCOwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTx3dTydR48AD0vgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4NnFy+V59AjABwIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECz90cT+fRI8BDE3j+aV3X/37c4/PAbQQeIOowegA6fv0+/Xn+8+kycBJgWWzw7OTvuH/2Gvh6Ag8QJfAAUQLPLj7ec3cPHsZbt20bPcOyLMsUQ/DeVx5fnOQ6hNnc9CWc4hSNc9C4BuDarYvPFIG3vc3JBg/fm3vwAFECDxAl8ABRAg8QJfAAUQIPECXwAFFTnINnTs6mw/dmgweIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiDqMHuDNOnoAgBobPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QNQrn0wuP5r0DXsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "elapse_time = 0\n",
    "total_reward = 0\n",
    "while True & (elapse_time < 100000):\n",
    "    print(state)\n",
    "    state = torch.from_numpy(state).float().to(torch_devices)\n",
    "    with torch.no_grad():\n",
    "        action = agent(state)\n",
    "    action = int(np.argmax(action))\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    elapse_time+=1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
