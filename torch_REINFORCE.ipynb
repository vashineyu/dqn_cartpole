{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of policy-CEM-method\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "_C = CN()\n",
    "_C.SYSTEM = CN()\n",
    "_C.SYSTEM.DEVICES = []\n",
    "_C.SYSTEM.VIRTUAL_SCREEN = (600, 400)\n",
    "_C.SYSTEM.PRINT_EVERY = 10\n",
    "\n",
    "_C.AGENT = CN()\n",
    "_C.AGENT.MAX_N_STEPS = 1000\n",
    "_C.AGENT.GAMMA = 1.0\n",
    "_C.AGENT.LR = 0.001\n",
    "\n",
    "_C.GAME_ENV = CN()\n",
    "_C.GAME_ENV.N_ITERATIONS = 2000\n",
    "_C.GAME_ENV.NOISE_SIGMA = 0.5\n",
    "\n",
    "def get_cfg_defaults():\n",
    "    return _C.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "from pyvirtualdisplay import Display\n",
    "from collections import deque\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "#from cartpole.config import get_cfg_defaults\n",
    "#from cartpole.utils import ReplayMemory, screen_to_state\n",
    "cfg = get_cfg_defaults()\n",
    "\n",
    "devices = \",\".join(str(i) for i in cfg.SYSTEM.DEVICES)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "\n",
    "torch_devices = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=cfg.SYSTEM.VIRTUAL_SCREEN)\n",
    "display.start()\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "#env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Agent_REINFORCE(nn.Module):\n",
    "    def __init__(self, input_size, output_size, device=\"cpu\"):\n",
    "        super(Agent_REINFORCE, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, 32)\n",
    "        self.layer2 = nn.Linear(32, 64)\n",
    "        self.layer3 = nn.Linear(64, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return F.softmax(self.layer3(x), dim=0) # action = 0 or 1 (discrete)\n",
    "    \n",
    "    \n",
    "    def get_weights_dim(self):\n",
    "        # return numbers of parameters of layers --> n in flatten form\n",
    "        return [len(param.reshape(-1).detach().numpy()) for param in self.parameters()]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        new_network_state = self._pair_weight_key_to_weight(weights=weights)\n",
    "        self.load_state_dict(new_network_state)\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, return a action and its logloss\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        probs = self.forward(state)\n",
    "        \n",
    "        sampler = torch.distributions.Categorical(probs) # kind of random.sample(prob)\n",
    "        action = sampler.sample()\n",
    "        \n",
    "        return action.item(), sampler.log_prob(action)\n",
    "        \n",
    "    \"\"\"\n",
    "    def evaluate(self, env, weights, gamma=1.0, max_n_steps=5000):\n",
    "        self.set_weights(weights)\n",
    "        state = env.reset()\n",
    "        rewards = 0.\n",
    "        for t in range(max_n_steps):\n",
    "            state = torch.from_numpy(state).float().to(self.device)\n",
    "            action = self.forward(state).detach().numpy()\n",
    "            #action = int(np.round(action)) # cartpole-v0 only accept 0 or 1 (no float between 0-1)\n",
    "            action = np.argmax(action) # cartpole-v0 only accept 0 or 1 (no float between 0-1)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards += reward * gamma**t\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        return rewards\n",
    "    \"\"\" \n",
    "    \n",
    "    def _pair_weight_key_to_weight(self, weights):\n",
    "        \"\"\"pair key and weights\n",
    "        Args:\n",
    "          weights: list of 1D-vectors n * [n_weights]\n",
    "          \n",
    "        Returns: A reshaped, mapped state_dict\n",
    "        \"\"\"\n",
    "        keys = list(self.state_dict().keys())\n",
    "        original_weight_shape = [list(param.shape) for param in self.parameters()]\n",
    "        return OrderedDict({k:torch.from_numpy(v).reshape(s) for k, v, s in zip(keys, weights, original_weight_shape)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent_REINFORCE(input_size=env.observation_space.shape[0], \n",
    "                        output_size=env.action_space.n, \n",
    "                        device=torch_devices).to(torch_devices)\n",
    "#optimizer = optim.Adam(agent.parameters(), lr=cfg.AGENT.LR)\n",
    "optimizer = optim.RMSprop(agent.parameters(), lr=cfg.AGENT.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_episode=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episode+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            action, log_prob = agent.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # gamma=1 means focus on now, and discount former experiences, so we reverse it\n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)][::-1] \n",
    "        R = sum([a*b for a,b in zip(rewards, discounts)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print(\"Episode {} Average Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Score: 25.12\n",
      "Episode 200 Average Score: 26.67\n",
      "Episode 300 Average Score: 35.63\n",
      "Episode 400 Average Score: 36.77\n",
      "Episode 500 Average Score: 49.61\n",
      "Episode 600 Average Score: 51.77\n",
      "Episode 700 Average Score: 53.78\n",
      "Episode 800 Average Score: 67.93\n",
      "Episode 900 Average Score: 93.27\n",
      "Episode 1000 Average Score: 117.53\n",
      "Episode 1100 Average Score: 113.85\n",
      "Episode 1200 Average Score: 124.85\n",
      "Episode 1300 Average Score: 137.93\n",
      "Episode 1400 Average Score: 133.08\n",
      "Episode 1500 Average Score: 133.34\n",
      "Episode 1600 Average Score: 150.12\n",
      "Episode 1700 Average Score: 150.86\n",
      "Episode 1800 Average Score: 177.13\n",
      "Episode 1900 Average Score: 182.46\n",
      "Episode 2000 Average Score: 165.46\n"
     ]
    }
   ],
   "source": [
    "scores = run(n_episode=cfg.GAME_ENV.N_ITERATIONS, \n",
    "             max_t=cfg.AGENT.MAX_N_STEPS, \n",
    "             gamma=cfg.AGENT.GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training, we examine the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABDlJREFUeJzt3OFpIlEYQNGZxSZSh21sHaYmrWPbSB1bxuRPWDYxCwuOeS/Xc0BQQfh+jJeP4em6bdsCQM+P0QMAcB8CDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRh9EDvPFzWoBr6y0ftsEDRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QdRg9Az8vl+eq94+k8YBJ4bDZ4diXuMA+BB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogWc3n/1VMDCOwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTx3dTydR48AD0vgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4NnFy+V59AjABwIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECz90cT+fRI8BDE3j+aV3X/37c4/PAbQQeIOowegA6fv0+/Xn+8+kycBJgWWzw7OTvuH/2Gvh6Ag8QJfAAUQLPLj7ec3cPHsZbt20bPcOyLMsUQ/DeVx5fnOQ6hNnc9CWc4hSNc9C4BuDarYvPFIG3vc3JBg/fm3vwAFECDxAl8ABRAg8QJfAAUQIPECXwAFFTnINnTs6mw/dmgweIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiDqMHuDNOnoAgBobPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QNQrn0wuP5r0DXsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "elapse_time = 0\n",
    "total_reward = 0\n",
    "while True & (elapse_time < 100000):\n",
    "    print(state)\n",
    "    state = torch.from_numpy(state).float().to(torch_devices)\n",
    "    with torch.no_grad():\n",
    "        action = agent(state)\n",
    "    action = int(np.argmax(action))\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    elapse_time+=1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
