{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of policy-CEM-method\n",
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "_C = CN()\n",
    "_C.SYSTEM = CN()\n",
    "_C.SYSTEM.DEVICES = []\n",
    "_C.SYSTEM.VIRTUAL_SCREEN = (600, 400)\n",
    "_C.SYSTEM.PRINT_EVERY = 10\n",
    "\n",
    "_C.AGENT = CN()\n",
    "_C.AGENT.MAX_N_STEPS = 500\n",
    "_C.AGENT.GAMMA = 1.0\n",
    "_C.AGENT.POPULATION_SIZE = 50\n",
    "_C.AGENT.ELITE_FRAC = 0.2\n",
    "\n",
    "_C.GAME_ENV = CN()\n",
    "_C.GAME_ENV.N_ITERATIONS = 30\n",
    "_C.GAME_ENV.NOISE_SIGMA = 0.5\n",
    "\n",
    "def get_cfg_defaults():\n",
    "    return _C.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "from pyvirtualdisplay import Display\n",
    "from collections import deque\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "#from cartpole.config import get_cfg_defaults\n",
    "#from cartpole.utils import ReplayMemory, screen_to_state\n",
    "cfg = get_cfg_defaults()\n",
    "\n",
    "devices = \",\".join(str(i) for i in cfg.SYSTEM.DEVICES)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "\n",
    "torch_devices = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=cfg.SYSTEM.VIRTUAL_SCREEN)\n",
    "display.start()\n",
    "env = gym.make(\"CartPole-v0\").unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "xx = env.render(mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Agent_PolicyCEM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, device=\"cpu\"):\n",
    "        super(Agent_PolicyCEM, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, 32)\n",
    "        self.layer2 = nn.Linear(32, 64)\n",
    "        self.layer3 = nn.Linear(64, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return torch.sigmoid(self.layer3(x)) # action = 0 or 1 (discrete, cannot be continous)\n",
    "    \n",
    "    def get_weights_dim(self):\n",
    "        # return numbers of parameters of layers --> n in flatten form\n",
    "        return [len(param.reshape(-1).detach().numpy()) for param in self.parameters()]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        new_network_state = self._pair_weight_key_to_weight(weights=weights)\n",
    "        self.load_state_dict(new_network_state)\n",
    "    \n",
    "    def evaluate(self, env, weights, gamma=1.0, max_n_steps=5000):\n",
    "        self.set_weights(weights)\n",
    "        state = env.reset()\n",
    "        rewards = 0.\n",
    "        for t in range(max_n_steps):\n",
    "            state = torch.from_numpy(state).float().to(self.device)\n",
    "            action = self.forward(state).detach().numpy()\n",
    "            action = int(np.round(action)) # cartpole-v0 only accept 0 or 1 (no float between 0-1)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards += reward + gamma**t\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        return rewards\n",
    "        \n",
    "    \n",
    "    def _pair_weight_key_to_weight(self, weights):\n",
    "        \"\"\"pair key and weights\n",
    "        Args:\n",
    "          weights: list of 1D-vectors n * [n_weights]\n",
    "          \n",
    "        Returns: A reshaped, mapped state_dict\n",
    "        \"\"\"\n",
    "        keys = list(self.state_dict().keys())\n",
    "        original_weight_shape = [list(param.shape) for param in self.parameters()]\n",
    "        return OrderedDict({k:torch.from_numpy(v).reshape(s) for k, v, s in zip(keys, weights, original_weight_shape)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent_PolicyCEM(input_size=4, output_size=1, device=torch_devices).to(torch_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 106.00\n",
      "Episode 20\tAverage Score: 240.90\n",
      "Episode 30\tAverage Score: 405.67\n",
      "\n",
      "Finally!\tAverage Score: 405.67\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "n_elite=int(cfg.AGENT.POPULATION_SIZE * cfg.AGENT.ELITE_FRAC)\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores = []\n",
    "best_weight = [cfg.GAME_ENV.NOISE_SIGMA * np.random.randn(i) for i in agent.get_weights_dim()]\n",
    "\n",
    "for i_iteration in range(1, cfg.GAME_ENV.N_ITERATIONS+1):\n",
    "    weights_pop = [[best_weight[i] + (cfg.GAME_ENV.NOISE_SIGMA * np.random.randn(j)) for i,j in enumerate(agent.get_weights_dim())] \\\n",
    "                    for _ in range(cfg.AGENT.POPULATION_SIZE)]\n",
    "    rewards = np.array([agent.evaluate(env=env, \n",
    "                                       weights=weights, \n",
    "                                       gamma=cfg.AGENT.GAMMA, \n",
    "                                       max_n_steps=cfg.AGENT.MAX_N_STEPS) for weights in weights_pop])\n",
    "\n",
    "    elite_idxs = rewards.argsort()[-n_elite:]\n",
    "    elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "    best_weight = np.array(elite_weights).mean(axis=0)\n",
    "\n",
    "    reward = agent.evaluate(env=env, weights=best_weight, gamma=cfg.AGENT.GAMMA, max_n_steps=5000)\n",
    "    scores_deque.append(reward)\n",
    "    scores.append(reward)\n",
    "\n",
    "    #torch.save(agent.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    if i_iteration % cfg.SYSTEM.PRINT_EVERY == 0:\n",
    "        print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "\n",
    "    \n",
    "print('\\nFinally!\\tAverage Score: {:.2f}'.format(np.mean(scores_deque)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.round(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABBFJREFUeJzt27tNQ0EURdE3yE1QB21Qh6kJ6qAN6qCMRwIEfCQ+tma8tVbkwEgn2rqyhrHv+wZAz9XsAQCch8ADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QNRh9oBX/p0W4LPxnz92wQNECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRB1mD9i2bXt6uHv/fHO8n7gEoMMFDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFGH2QM+GmP8+Lv7vp9xCcBlc8EDRC0R+Mfn4/b4fJw9AyBlicC/EXmA01kq8ACcjsADRC0V+Nvrh9kTADLGCk8Nxxh/GrHCdoAz+vm78S8s9w7+N37zZh7g0vz3iL3owLvgAb631G/wAJyOwANECTxAlMADRAk8QJTAA0QJPEDUEu/gvWcHOD0XPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxA1GH2gFdj9gCAGhc8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxA1AvL8yL81tyjDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "while True:\n",
    "    state = torch.from_numpy(state).float().to(torch_devices)\n",
    "    with torch.no_grad():\n",
    "        action = agent(state)\n",
    "    action = int(np.round(action))\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
