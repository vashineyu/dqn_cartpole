{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "from pyvirtualdisplay import Display\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "from cartpole.config import get_cfg_defaults\n",
    "from cartpole.utils import ReplayMemory, screen_to_state\n",
    "#from cartpole.model import DQN\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "\n",
    "devices = \",\".join(str(i) for i in cfg.SYSTEM.DEVICES)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "\n",
    "torch_devices = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you want to access the behind-the.scenes dynamics of a specific environment, \n",
    "then you use the unwrapped property.\n",
    "\"\"\"\n",
    "display = Display(visible=0, size=cfg.SYSTEM.VIRTUAL_SCREEN)\n",
    "display.start()\n",
    "env = gym.make(\"CartPole-v0\").unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        h,w, *_ = input_size\n",
    "        conv_w_out = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, kernel_size=3, stride=2), \n",
    "                                                     kernel_size=3, stride=2), \n",
    "                                     kernel_size=3, stride=2)\n",
    "        conv_h_out = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, kernel_size=3, stride=2), \n",
    "                                                     kernel_size=3, stride=2), \n",
    "                                     kernel_size=3, stride=2)\n",
    "        \n",
    "        linear_input_size = conv_w_out * conv_h_out * 64\n",
    "        self.head = nn.Linear(linear_input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "        \n",
    "        \n",
    "def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from cartpole.utils import Transition, ReplayMemory\n",
    "import random\n",
    "\n",
    "class DqnAgent():\n",
    "    def __init__(self, \n",
    "                 state_shape, action_space,\n",
    "                 device,\n",
    "                 soft_update_ratio=0.01, \n",
    "                 learning_rate=1e-4, \n",
    "                 gamma=0.99, \n",
    "                 batch_size=128,\n",
    "                 update_every = 10,\n",
    "                 memory_size=10000\n",
    "                 ):\n",
    "        self.state_shape = state_shape,\n",
    "        self.action_space = action_space\n",
    "        self.soft_update_ratio = soft_update_ratio\n",
    "        self.gamma = gamma\n",
    "        self.update_every = update_every\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        # -- init -- #\n",
    "        self.q_policy_net = DQN(input_size=state_shape, output_size=action_space).to(device)\n",
    "        self.q_target_net = DQN(input_size=state_shape, output_size=action_space).to(device)\n",
    "        self.q_target_net.load_state_dict(self.q_policy_net.state_dict()) # sync weights\n",
    "        self.q_target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.q_policy_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, next_state, reward, is_done):\n",
    "        \"\"\"Add memory and learn\"\"\"\n",
    "        self.memory.push(state, action, next_state, reward, is_done*1.)\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experience = self.memory.sample(self.batch_size)\n",
    "            batch = Transition(*zip(*experience))\n",
    "\n",
    "            state = torch.tensor(batch.state).permute(0, 3, 1, 2)\n",
    "            action = torch.tensor(batch.action)\n",
    "            reward = torch.tensor(batch.reward)\n",
    "            next_state = torch.tensor(batch.next_state).permute(0, 3, 1, 2)\n",
    "            is_done = torch.tensor(batch.done)\n",
    "\n",
    "            q_targets_next = self.q_target_net(next_state).max(1)[0].detach() # Q(s', a)\n",
    "            q_targets = reward + (self.gamma * q_targets_next * (1 - is_done)) # R + gamma*Q(s',a)\n",
    "\n",
    "            q_expected = self.q_policy_net(state).gather(1, action.unsqueeze(1)) # Q(s,a)\n",
    "\n",
    "            # optimize: l = R + gamma*Q(s', a) - Q(s, a)\n",
    "            loss = F.smooth_l1_loss(q_expected, q_targets.unsqueeze(1)) # batch x 1\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.t_step = (self.t_step + 1) % self.update_every\n",
    "            if self.t_step == 0:\n",
    "                self._soft_update(target_model=self.q_target_net,\n",
    "                                  local_model=self.q_policy_net,\n",
    "                                  tau=self.soft_update_ratio)\n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Generate actions\"\"\"\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state[np.newaxis, :,:,:]).permute(0, 3, 1, 2)\n",
    "            self.q_policy_net.eval() # swap to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_policy_net(state)\n",
    "            self.q_policy_net.train() # swap to training mode\n",
    "            \n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_space))\n",
    "    \n",
    "    def _soft_update(self, target_model, local_model, tau):\n",
    "        for target_params, local_params in zip(target_model.parameters(), loacl_model.parameters()):\n",
    "            target_params.data.copy_(tau*local_params.data + (1.-tau)*target_params.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(capacity=int(cfg.AGENT.NUM_MEMORY_CAPACITY))\n",
    "agent = DqnAgent(state_shape=cfg.MODEL.INPUT_SIZE,\n",
    "                 action_space=env.action_space.n, \n",
    "                 device=torch_devices,\n",
    "                 gamma=cfg.AGENT.GAMMA, \n",
    "                 batch_size=cfg.AGENT.BATCH_SIZE, \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def process_image(state_image, target_size):\n",
    "    w, h, *_ = target_size\n",
    "    state = cv2.resize(state_image, (w, h))\n",
    "    state = state / 255.\n",
    "    return state\n",
    "    \n",
    "#test_im = env.render(mode='rgb_array')\n",
    "#test_im = process_image(test_im, (128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "xx = env.render(mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((128, 128, 3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Accumulated Reward: 39.0, passed n mean reward: 39.0\n",
      "Episode 1, Accumulated Reward: 37.0, passed n mean reward: 38.0\n",
      "Episode 2, Accumulated Reward: 20.0, passed n mean reward: 32.0\n",
      "Episode 3, Accumulated Reward: 14.0, passed n mean reward: 27.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-41d3b59c0b2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(\"\\rEpisode {}, Accumulated Reward: {:.3f}, remain time: {}\".format(i_episode, total_rewards, t_counter), end='')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-9dd7c2beee88>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, next_state, reward, is_done)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyttf/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyttf/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#cfg.AGENT.NUM_EPISODE\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "eps = cfg.AGENT.EPS_START\n",
    "for i_episode in range(cfg.AGENT.NUM_EPISODE):\n",
    "    env.reset()\n",
    "    last_screen = screen_to_state(env, target_size=cfg.MODEL.INPUT_SIZE[:2])\n",
    "    current_screen = screen_to_state(env, target_size=cfg.MODEL.INPUT_SIZE[:2])\n",
    "    state = current_screen - last_screen\n",
    "    total_rewards = 0\n",
    "    for t_counter in range(cfg.AGENT.MAX_T):\n",
    "        action = agent.act(state, eps)\n",
    "        \n",
    "        vector_state, reward, is_done, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "        \n",
    "        last_screen = current_screen\n",
    "        current_screen = screen_to_state(env, target_size=cfg.MODEL.INPUT_SIZE[:2])\n",
    "        if not is_done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = current_screen - current_screen\n",
    "        state = next_state\n",
    "        \n",
    "        # Train the model\n",
    "        agent.step(state, action, next_state, reward, is_done*1)\n",
    "        #print(\"\\rEpisode {}, Accumulated Reward: {:.3f}, remain time: {}\".format(i_episode, total_rewards, t_counter), end='')\n",
    "        \n",
    "        if is_done:\n",
    "            break\n",
    "    scores.append(total_rewards)\n",
    "    scores_window.append(total_rewards)\n",
    "    print(\"\\rEpisode {}, Accumulated Reward: {:.1f}, passed n mean reward: {:.1f}\".format(i_episode, \n",
    "                                                                                          total_rewards, \n",
    "                                                                                          np.mean(scores_window)\n",
    "                                                                                         ))\n",
    "    eps = max(cfg.AGENT.EPS_END, eps*cfg.AGENT.EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.tensor(batch.state).permute(0, 3, 1, 2)\n",
    "ns = torch.tensor(batch.next_state).permute(0, 3, 1, 2)\n",
    "r = torch.tensor(batch.reward)\n",
    "d = torch.tensor(batch.done)\n",
    "a = torch.tensor(batch.action)\n",
    "\n",
    "vnxt = agent.q_target(ns).max(1)[0].detach()\n",
    "vt = r + (0.9 * vnxt * (1 - d))\n",
    "qe = agent.q_policy(s).gather(1, a.unsqueeze(1))\n",
    "\n",
    "#q_targets_next = self.q_target(next_state).max(axis=1)[0].detach()\n",
    "#q_targets = reward + (gamma * q_targets_next * (1 - is_done))\n",
    "#q_expected = self.policy_net(state).gather(1, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = memory.sample(32)\n",
    "batch = Transition(*zip(*b))\n",
    "\n",
    "#state = torch.cat(batch.state)\n",
    "#action = torch.cat(batch.action)\n",
    "#reward = torch.cat(batch.reward)\n",
    "#next_state = torch.cat(batch.next_state)\n",
    "#is_done = torch.cat(batch.is_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(batch.state)\n",
    "#t.permute(0,3,1,2).shape\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.permute(0, 3, 1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Transition(*zip(*m))\n",
    "batch = [np.array(b.state, np.float32), \n",
    "         np.array(b.action, dtype=np.float32), \n",
    "         np.array(b.next_state, dtype=np.float32),\n",
    "         np.array(b.reward, dtype=np.float32), \n",
    "         np.array(b.done, dtype=np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_brain = Brain(policy_net=agent.policy_net, target_net=agent.target_net, gamma=0.9)\n",
    "optim = tf.keras.optimizers.Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    return K.square(K.mean(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = new_brain.policy_net.layers[0]\n",
    "for pl,tl in zip(new_brain.policy_net.layers, new_brain.target_net.layers):\n",
    "    #for wp, tp in zip(pl.get_weights(), tl.get_weights()):\n",
    "    #    pass\n",
    "    pl.set_weights([wp+tp for wp,tp in zip(pl.get_weights(), tl.get_weights())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_brain.policy_net.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_brain.target_net.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Not implementedError ...\n",
    "with tf.GradientTape() as tape:\n",
    "    target, estimate = new_brain(batch)\n",
    "    loss = compute_loss(target, estimate)    \n",
    "grads = tape.gradient(loss, new_brain.trainable_variables)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, estimate = new_brain(batch)\n",
    "loss = compute_loss(target, estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tf.gradients(loss, new_brain.trainable_variables)\n",
    "_ = optim.apply_gradients(zip(grads, new_brain.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_brain.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(screen_to_state(env))\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(\"\\r{}\".format(i), end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
